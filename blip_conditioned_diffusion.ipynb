{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fdd8bf-f54d-4b10-8296-c8cfd906c3f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3ed8b-9164-4f87-ae77-a2ecf8c9243c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dff3ed8b-9164-4f87-ae77-a2ecf8c9243c",
    "outputId": "0b7402f3-8013-4dbd-ab86-10c6acb508fb"
   },
   "outputs": [],
   "source": [
    "!wget https://r2-public-worker.drysys.workers.dev/sd-v1-4-full-ema.ckpt\n",
    "!git clone https://github.com/CompVis/stable-diffusion.git\n",
    "%cd stable-diffusion\n",
    "!wget https://raw.githubusercontent.com/justinpinkney/stable-diffusion/main/requirements.txt\n",
    "!pip install -r requirements.txt\n",
    "!pip install --upgrade pytorch-lightning\n",
    "!apt-get update -y && apt-get install libgl1 -y && apt-get install libglib2.0-0 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388be936-5478-4485-8c98-58a93a6d3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ftfy regex tqdm timm==0.4.12 fairscale==0.4.4\n",
    "!pip3 install git+https://github.com/openai/CLIP.git\n",
    "!git clone https://github.com/pharmapsychotic/clip-interrogator.git\n",
    "!git clone https://github.com/salesforce/BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e96d11fb-9dc4-4f57-b9e9-8c3b0a50556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silly hack you need to do:\n",
    "# apt install vim -y\n",
    "# vim /opt/conda/lib/python3.7/site-packages/transformers/generation_utils.py\n",
    "# Delete line 1146 (validating kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70f13d-a2fc-42d8-9be2-5a3ec83cb76b",
   "metadata": {
    "id": "e13b0f4e-8b77-452a-ab1e-2d1fb3822862"
   },
   "source": [
    "## !! Restart your notebook here !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225053c-2be5-412f-8966-196fe92f0417",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "UfRggL1RZin8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UfRggL1RZin8",
    "outputId": "9c5b13a9-b0bb-4277-d94e-3fcd03afe997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/BLIP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c1e7d0efa014fd2859c72674f84e52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3490302bff4593a5b262dfd52af0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa699666546d4f7fbce4d8f26c02350b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa630675b2e648d8baa70d7fc193b31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/855M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth\n",
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "#@title Setup\n",
    "%cd BLIP\n",
    "\n",
    "import clip\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from models.blip import blip_decoder\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "blip_image_eval_size = 384\n",
    "blip_model_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model*_base_caption.pth'        \n",
    "blip_model = blip_decoder(pretrained=blip_model_url, image_size=blip_image_eval_size, vit='base')\n",
    "blip_model.eval()\n",
    "blip_model = blip_model.to(device)\n",
    "\n",
    "def generate_caption(pil_image):\n",
    "    gpu_image = transforms.Compose([\n",
    "        transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "    ])(pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        caption = blip_model.generate(gpu_image, sample=False, num_beams=3, max_length=20, min_length=5)\n",
    "    return caption[0]\n",
    "\n",
    "def load_list(filename):\n",
    "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
    "        items = [line.strip() for line in f.readlines()]\n",
    "    return items\n",
    "\n",
    "def rank(model, image_features, text_array, top_count=1):\n",
    "    top_count = min(top_count, len(text_array))\n",
    "    text_tokens = clip.tokenize([text for text in text_array]).cuda()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = torch.zeros((1, len(text_array))).to(device)\n",
    "    for i in range(image_features.shape[0]):\n",
    "        similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
    "    similarity /= image_features.shape[0]\n",
    "\n",
    "    top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
    "    return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
    "\n",
    "def interrogate(image, models):\n",
    "    caption = generate_caption(image)\n",
    "    if len(models) == 0:\n",
    "        print(f\"\\n\\n{caption}\")\n",
    "        return\n",
    "\n",
    "    table = []\n",
    "    bests = [[('',0)]]*5\n",
    "    for model_name in models:\n",
    "        print(f\"Interrogating with {model_name}...\")\n",
    "        model, preprocess = clip.load(model_name)\n",
    "        model.cuda().eval()\n",
    "\n",
    "        images = preprocess(image).unsqueeze(0).cuda()\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(images).float()\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        ranks = [\n",
    "            rank(model, image_features, mediums),\n",
    "            rank(model, image_features, [\"by \"+artist for artist in artists]),\n",
    "            rank(model, image_features, trending_list),\n",
    "            rank(model, image_features, movements),\n",
    "            rank(model, image_features, flavors, top_count=3)\n",
    "        ]\n",
    "\n",
    "        for i in range(len(ranks)):\n",
    "            confidence_sum = 0\n",
    "            for ci in range(len(ranks[i])):\n",
    "                confidence_sum += ranks[i][ci][1]\n",
    "            if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
    "                bests[i] = ranks[i]\n",
    "\n",
    "        row = [model_name]\n",
    "        for r in ranks:\n",
    "            row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
    "\n",
    "        table.append(row)\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "    #display(pd.DataFrame(table, columns=[\"Model\", \"Medium\", \"Artist\", \"Trending\", \"Movement\", \"Flavors\"]))\n",
    "\n",
    "    flaves = ', '.join([f\"{x[0]}\" for x in bests[4]])\n",
    "    medium = bests[0][0][0]\n",
    "    if caption.startswith(medium):\n",
    "        out = f\"{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n",
    "        print(f\"\\n\\n{caption} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\")\n",
    "    else:\n",
    "        out = f\"{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\"\n",
    "        print(f\"\\n\\n{caption}, {medium} {bests[1][0][0]}, {bests[2][0][0]}, {bests[3][0][0]}, {flaves}\")\n",
    "    return caption, out\n",
    "\n",
    "data_path = \"../clip-interrogator/data/\"\n",
    "\n",
    "artists = load_list(os.path.join(data_path, 'artists.txt'))\n",
    "flavors = load_list(os.path.join(data_path, 'flavors.txt'))\n",
    "mediums = load_list(os.path.join(data_path, 'mediums.txt'))\n",
    "movements = load_list(os.path.join(data_path, 'movements.txt'))\n",
    "\n",
    "sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
    "trending_list = [site for site in sites]\n",
    "trending_list.extend([\"trending on \"+site for site in sites])\n",
    "trending_list.extend([\"featured on \"+site for site in sites])\n",
    "trending_list.extend([site+\" contest winner\" for site in sites])\n",
    "\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcac723-21c4-4ed1-9c3e-e62b75f44dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd stable-diffusion\n",
    "# Waifu Diffusion\n",
    "#ckpt_file = \"wd-v1-2-full-ema.ckpt\"\n",
    "\n",
    "#Stable Diffusion\n",
    "ckpt_file = \"sd-v1-4-full-ema.ckpt\"\n",
    "\n",
    "#Poke Diffusion\n",
    "#ckpt_file = \"pokediffusion_epoch_10_pruned.ckpt\"\n",
    "\n",
    "from io import BytesIO\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "from einops import repeat\n",
    "import PIL\n",
    "\n",
    "import fire\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from torch import autocast\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from ldm.util import instantiate_from_config\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "def load_img(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.*image - 1.\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "config = OmegaConf.load(\"configs/stable-diffusion/v1-inference.yaml\")\n",
    "model = load_model_from_config(config, f\"../{ckpt_file}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "start_code = None\n",
    "\n",
    "%cd ..\n",
    "\n",
    "sample_path = \"outs\"\n",
    "os.makedirs(sample_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc675cf8-0845-4a06-a6e2-fb5a11006f8b",
   "metadata": {},
   "source": [
    "## Set up parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "iZWwv6xVaHQL",
   "metadata": {
    "id": "iZWwv6xVaHQL"
   },
   "outputs": [],
   "source": [
    "#@title Generate!\n",
    "\n",
    "#@markdown \n",
    "\n",
    "#@markdown #####**Base Settings:**\n",
    "\n",
    "prompt = \"A psychedelic being living in an extradimensional reality, in the style of wlop, illustration, epic, fantasy, hyper detailed, smooth, unreal engine, sharp focus, ray tracing, physically based rendering, renderman, beautiful\" #@param {type:\"string\"}\n",
    "image_path_or_url = \"https://scontent-sjc3-1.xx.fbcdn.net/v/t1.15752-9/308282132_472221518278262_4631038285071535282_n.png?_nc_cat=111&ccb=1-7&_nc_sid=ae9488&_nc_ohc=9JUwOq6AGrEAX8hOOcz&_nc_ht=scontent-sjc3-1.xx&oh=03_AVJg2VM4JOsFtlpDoGN3zqnLcmGRcIpFApdj3YAsFdaCJA&oe=63597747\" #@param {type:\"string\"}\n",
    "seed = 123 #@param {type:\"number\"}\n",
    "ddim_steps = 50\n",
    "\n",
    "if str(image_path_or_url).startswith('http://') or str(image_path_or_url).startswith('https://'):\n",
    "    image = Image.open(requests.get(image_path_or_url, stream=True).raw).convert('RGB')\n",
    "else:\n",
    "    image = Image.open(image_path_or_url).convert('RGB')\n",
    "\n",
    "#@markdown #####**Img2Img settings:**\n",
    "\n",
    "img2img_type = \"clip-interrogator\" #@param [\"basic\", \"blip\", \"clip-interrogator\"]\n",
    "img_strength = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
    "img_strength = 1 - img_strength \n",
    "blip_strength = 0.4 #@param {type:\"slider\", min:0, max:1, step:0.01}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5117a72-9e64-46b9-b96a-d89341301ea5",
   "metadata": {},
   "source": [
    "## Generate! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e961d5b7-e524-4eb8-b740-3e314ac7aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrogating with ViT-L/14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 890M/890M [02:49<00:00, 5.49MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "a picture of a cat in a space suit, a stock photo by Pogus Caesar, featured on reddit, space art, futuristic, sci-fi, stock photo\n"
     ]
    }
   ],
   "source": [
    "blip_prompt, clip_inter_prompt = interrogate(image, models=[\"ViT-L/14\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "-HYttO9Fda2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HYttO9Fda2e",
    "outputId": "871bd952-f629-40c6-a723-e17fd239336c"
   },
   "outputs": [],
   "source": [
    "img_cond = None\n",
    "if img2img_type == \"blip\":\n",
    "    img_cond = model.get_learned_conditioning([blip_prompt])\n",
    "elif img2img_type == \"clip-interrogator\":\n",
    "    img_cond = model.get_learned_conditioning([clip_inter_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07fc3ab-269f-4a12-a08e-5a314c845f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8E8pReBScHnL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "8E8pReBScHnL",
    "outputId": "75dc7807-c407-4b23-a9b6-56306bef1dc3"
   },
   "outputs": [],
   "source": [
    "init_image = load_img(image).to(\"cuda\")\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=1)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "\n",
    "batch_size = 1\n",
    "data = [batch_size * [prompt]]\n",
    "\n",
    "\n",
    "for s in range(10):\n",
    "    t_enc = int(img_strength * ddim_steps)\n",
    "    incr_seed = s + seed\n",
    "    sampler.make_schedule(ddim_num_steps=50, ddim_eta=0.0, verbose=False)\n",
    "\n",
    "    seed_everything(incr_seed)\n",
    "    precision_scope = autocast\n",
    "    with torch.no_grad():\n",
    "            with precision_scope(\"cuda\"):\n",
    "                with model.ema_scope():\n",
    "                    all_samples = list()\n",
    "                    for prompts in data:\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                        if img_cond != None and blip_strength >= 0:\n",
    "                            c = torch.lerp(c, img_cond, blip_strength)\n",
    "\n",
    "                        # encode (scaled latent)\n",
    "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                        # decode it\n",
    "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=7.5,\n",
    "                                                  unconditional_conditioning=uc,)\n",
    "\n",
    "                        x_samples = model.decode_first_stage(samples)\n",
    "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                        for x_sample in x_samples:\n",
    "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                            Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                os.path.join(sample_path, f\"{incr_seed}_{str(uuid.uuid4())}.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a544bb-37da-40be-ada9-aee561acce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid version\n",
    "\n",
    "init_image = load_img(image).to(\"cuda\")\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=1)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))\n",
    "\n",
    "batch_size = 1\n",
    "data = [batch_size * [prompt]]\n",
    "\n",
    "\n",
    "grid_img_strs = [0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "grid_blip_strs = [0, 0.1, 0.25, 0.4, 0.55, 0.7, 0.9]\n",
    "\n",
    "for s in range(10):\n",
    "    all_samples = []\n",
    "    for img_strength in grid_img_strs:\n",
    "        for blip_strength in grid_blip_strs:\n",
    "            t_enc = int((1-img_strength) * ddim_steps)\n",
    "            incr_seed = seed + s\n",
    "            sampler.make_schedule(ddim_num_steps=50, ddim_eta=0.0, verbose=False)\n",
    "\n",
    "            seed_everything(incr_seed)\n",
    "            precision_scope = autocast\n",
    "            with torch.no_grad():\n",
    "                    with precision_scope(\"cuda\"):\n",
    "                        with model.ema_scope():\n",
    "                            for prompts in data:\n",
    "                                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                                if isinstance(prompts, tuple):\n",
    "                                    prompts = list(prompts)\n",
    "                                c = model.get_learned_conditioning(prompts)\n",
    "\n",
    "                                if img_cond != None and blip_strength >= 0:\n",
    "                                    c = torch.lerp(c, img_cond, blip_strength)\n",
    "\n",
    "                                # encode (scaled latent)\n",
    "                                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
    "                                # decode it\n",
    "                                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=7.5,\n",
    "                                                          unconditional_conditioning=uc,)\n",
    "\n",
    "                                x_samples = model.decode_first_stage(samples)\n",
    "                                x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                                for x_sample in x_samples:\n",
    "                                    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                                    Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                        os.path.join(sample_path, f\"{incr_seed}_{str(uuid.uuid4())}.png\"))\n",
    "                                    all_samples.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
    "    gap = 20\n",
    "    side = 7\n",
    "\n",
    "    w,h = all_samples[0].size\n",
    "\n",
    "    big_img_w = (side * w) + ((side - 1) * gap)\n",
    "    big_img_h = (side * h) + ((side - 1) * gap)\n",
    "\n",
    "    big_img = Image.new('RGB', (big_img_w, big_img_h))\n",
    "\n",
    "    for row in range(side):\n",
    "        for col in range(side):\n",
    "            ind = row + (col * side)\n",
    "            curr_w = (col * (w+gap))\n",
    "            curr_h = (row * (h+gap))\n",
    "            print(f\"Pasting at {curr_w}, {curr_h}\")\n",
    "            big_img.paste(all_samples[ind], (curr_w, curr_h))\n",
    "\n",
    "    big_img.save(os.path.join(sample_path, f\"big_image_{str(uuid.uuid4())}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87576290-6108-46ce-a8ea-f570988ede05",
   "metadata": {
    "id": "87576290-6108-46ce-a8ea-f570988ede05"
   },
   "outputs": [],
   "source": [
    "#concatenate images\n",
    "gap = 20\n",
    "side = 7\n",
    "\n",
    "w,h = all_samples[0].size\n",
    "\n",
    "big_img_w = (side * w) + ((side - 1) * gap)\n",
    "big_img_h = (side * h) + ((side - 1) * gap)\n",
    "\n",
    "big_img = Image.new('RGB', (big_img_w, big_img_h))\n",
    "\n",
    "for row in range(side):\n",
    "    for col in range(side):\n",
    "        ind = row + (col * side)\n",
    "        curr_w = (col * (w+gap))\n",
    "        curr_h = (row * (h+gap))\n",
    "        print(f\"Pasting at {curr_w}, {curr_h}\")\n",
    "        big_img.paste(all_samples[ind], (curr_w, curr_h))\n",
    "        \n",
    "big_img.save(os.path.join(sample_path, f\"big_image_{str(uuid.uuid4())}.png\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "affa318d82c20b47c0ef92062231e2c380c6e13132d56fecdf79ed1254aa3821"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
